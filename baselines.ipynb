{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a simple GMM and a non-recurrent mixture density network (i.e. an MDN) will be trained and evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Misc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle \n",
    "\n",
    "# Visualiation tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "from matplotlib import rc\n",
    "# Plot settings\n",
    "#plt.style.use('seaborn-dark')\n",
    "#plt.rcParams['figure.figsize'] = (10,4)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "# Latex font in plots\n",
    "rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Computer Modern Roman']})\n",
    "\n",
    "# Pyro/PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Predictive, NUTS, MCMC, config_enumerate\n",
    "from pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pyro.distributions import MultivariateNormal as MN\n",
    "from pyro.ops.indexing import Vindex\n",
    "\n",
    "# Implemented modules\n",
    "from util import *\n",
    "from model import RMDN, NonRMDN, RMDN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "X_train_tensor, X_val_tensor, X_test_tensor =   torch.load(\"./data/X_train_tensor_1h.pt\"), \\\n",
    "                                                torch.load(\"./data/X_val_tensor_1h.pt\"),   \\\n",
    "                                                torch.load(\"./data/X_test_tensor_1h.pt\")\n",
    "U_train_tensor, U_val_tensor, U_test_tensor =   torch.load(\"./data/U_train_tensor_1h.pt\"), \\\n",
    "                                                torch.load(\"./data/U_val_tensor_1h.pt\"),   \\\n",
    "                                                torch.load(\"./data/U_test_tensor_1h.pt\")\n",
    "N_t_train, N_t_valid, N_t_test = np.load(\"./data/N_t_train_1h.npy\"), \\\n",
    "                                 np.load(\"./data/N_t_val_1h.npy\"),   \\\n",
    "                                 np.load(\"./data/N_t_test_1h.npy\")\n",
    "\n",
    "# Concatenation\n",
    "X_train_val = torch.cat((X_train_tensor, X_val_tensor), 1)\n",
    "U_train_val = torch.cat((U_train_tensor, U_val_tensor), 1)\n",
    "N_train_val = np.hstack((N_t_train, N_t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training LL:\t-28020.4291\n",
      "Baseline validation LL:\t-15878.9001\n",
      "Baseline testing LL:\t-13550.5518\n"
     ]
    }
   ],
   "source": [
    "# Create input for mixture model\n",
    "X_train_full = np.zeros((np.sum(N_t_train), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_train):\n",
    "    X_train_full[prev_count:(prev_count+count), :] = X_train_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "    \n",
    "# Create validation set\n",
    "X_val_full = np.zeros((np.sum(N_t_valid), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_valid):\n",
    "    X_val_full[prev_count:(prev_count+count), :] = X_val_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "\n",
    "# Create test set\n",
    "X_test_full = np.zeros((np.sum(N_t_test), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_test):\n",
    "    X_test_full[prev_count:(prev_count+count), :] = X_test_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count \n",
    "\n",
    "# Compute boundaries\n",
    "latmax = (latMax-latmean)/latstd\n",
    "latmin = (latMin-latmean)/latstd\n",
    "lonmax = (lonMax-lonmean)/lonstd\n",
    "lonmin = (lonMin-lonmean)/lonstd\n",
    "\n",
    "# Bin the training data\n",
    "bins_lat = np.linspace(latmin, latmax, 33)\n",
    "bins_lon = np.linspace(lonmin, lonmax, 33)\n",
    "binidx_lat = np.digitize(X_train_full[:,1], bins=bins_lat)\n",
    "binidx_lon = np.digitize(X_train_full[:,0], bins=bins_lon)\n",
    "X_train_base = np.vstack((bins_lon[binidx_lon], bins_lat[binidx_lat]))    \n",
    "    \n",
    "# Initialise and fit model\n",
    "baseline = GaussianMixture(n_components=15).fit(X_train_base.T)\n",
    "\n",
    "# Compute loglikelihood\n",
    "baseline_ll_train = baseline.score(X_train_full) * np.sum(N_t_train)\n",
    "baseline_ll_validation = baseline.score(X_val_full) * np.sum(N_t_valid)\n",
    "baseline_ll_test = baseline.score(X_test_full) * np.sum(N_t_test)\n",
    "print(f'Baseline training LL:\\t{baseline_ll_train:.4f}')\n",
    "print(f'Baseline validation LL:\\t{baseline_ll_validation:.4f}')\n",
    "print(f'Baseline testing LL:\\t{baseline_ll_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-recurrent MDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-recurrent implementation of the mixture density network will here be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = RFNDataset(X_train_tensor, U_train_tensor)\n",
    "dataloader= DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "grid = U_train_tensor.size(3)\n",
    "model = NonRMDN(input_dim=grid, hidden_dim=128, LSTM_input=32, output_dim=2, LSTM_dim=32, K = 60)\n",
    "guide = model.guide \n",
    "optimizer = pyro.optim.ClippedAdam({\"lr\":3e-4, \"clip_norm\":5., \"weight_decay\":5e-2})\n",
    "svi = SVI(model.model, guide, optimizer, TraceEnum_ELBO(num_particles=1, max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, \tLoss: 3.103, \tTrain LL: -31753.396484375, \tValidation LL: -18028.830078125\n",
      "Epoch: 500, \tLoss: 2.513, \tTrain LL: -24929.599609375, \tValidation LL: -14318.6181640625\n",
      "Epoch: 1000, \tLoss: 2.402, \tTrain LL: -23938.142578125, \tValidation LL: -13927.0947265625\n",
      "Epoch: 1500, \tLoss: 2.362, \tTrain LL: -23520.77734375, \tValidation LL: -13805.193359375\n",
      "Epoch: 2000, \tLoss: 2.337, \tTrain LL: -23032.556640625, \tValidation LL: -13590.35546875\n",
      "Epoch: 2500, \tLoss: 2.312, \tTrain LL: -22744.783203125, \tValidation LL: -13554.66015625\n",
      "Epoch: 3000, \tLoss: 2.283, \tTrain LL: -22517.62109375, \tValidation LL: -13549.6533203125\n",
      "Epoch: 3500, \tLoss: 2.248, \tTrain LL: -22297.943359375, \tValidation LL: -13556.2412109375\n",
      "Epoch: 4000, \tLoss: 2.240, \tTrain LL: -22060.712890625, \tValidation LL: -13585.423828125\n",
      "Epoch: 4500, \tLoss: 2.229, \tTrain LL: -21886.8515625, \tValidation LL: -13609.41796875\n",
      "Epoch: 5000, \tLoss: 2.199, \tTrain LL: -21608.046875, \tValidation LL: -13587.185546875\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_epochs = 5000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_ll = []\n",
    "validation_ll =  []\n",
    "best_ll = -np.inf\n",
    "for i in range(num_epochs):\n",
    "    for X_i, U_i in dataloader:\n",
    "        # Set model to training\n",
    "        model.train()\n",
    "         # Take step and update parameters\n",
    "        loss = svi.step(X_i, U_i, N_t_train) / (N_t_train.sum())\n",
    "        # Save current loss\n",
    "        train_losses.append(loss)       \n",
    "        if i%500 == 499 or i == 0:\n",
    "            # Set model to evaluation\n",
    "            model.eval()\n",
    "            #Compute LL on training and validation set\n",
    "            train_ll_i      = model.get_loglikelihood(X=X_i, U=U_i, mask=N_t_train).sum()\n",
    "            validation_ll_i = model.get_loglikelihood(X=X_val_tensor, U=U_val_tensor, mask=N_t_valid).sum()\n",
    "            # Save LLs\n",
    "            train_ll.append(train_ll_i)\n",
    "            validation_ll.append(validation_ll_i)\n",
    "            \n",
    "            print(f\"Epoch: {i+1}, \\tLoss: {loss:.3f}, \\tTrain LL: {train_ll_i.numpy()}, \\tValidation LL: {validation_ll_i.numpy()}\")\n",
    "            \n",
    "            # Check if current LL best. If so, save checkpoint.\n",
    "            if validation_ll_i > best_ll:\n",
    "                best_ll = validation_ll_i\n",
    "                torch.save(model.state_dict(), 'models/MDN_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save losses, LLs and model if save == True\n",
    "save = True\n",
    "if save:\n",
    "    with open(\"models/mdn_losses_60.txt\", \"wb\") as fp:   \n",
    "        pickle.dump(train_losses, fp)\n",
    "\n",
    "    with open(\"models/mdn_validation_60.txt\", \"wb\") as fp:\n",
    "        pickle.dump(validation_ll, fp)\n",
    "\n",
    "    with open(\"models/mdn_train_60.txt\", \"wb\") as fp:\n",
    "        pickle.dump(train_ll, fp)    \n",
    "\n",
    "    torch.save(model.state_dict(), 'models/MDN_trained_60')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonRMDN(\n",
       "  (FeatureExtractor): FeatureExtractor(\n",
       "    (input_to_hidden): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_output): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (MDN): MDN(\n",
       "    (input_to_hidden): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_loc): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (hidden_to_sigma): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (hidden_to_off_diag): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (hidden_to_mix): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (softplus): Softplus(beta=1, threshold=20)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that an MDN has been trained, it is here loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonRMDN(\n",
       "  (FeatureExtractor): FeatureExtractor(\n",
       "    (input_to_hidden): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_output): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (MDN): MDN(\n",
       "    (input_to_hidden): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_loc): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (hidden_to_sigma): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (hidden_to_off_diag): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (hidden_to_mix): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (softplus): Softplus(beta=1, threshold=20)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and load model\n",
    "grid = U_train_tensor.size(3)\n",
    "model = NonRMDN(input_dim=grid, hidden_dim=128, LSTM_input=32, output_dim=2, LSTM_dim=32, K = 60)\n",
    "model.load_state_dict(torch.load('models/MDN_trained_60'))\n",
    "# Set to evaluation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log-likelihood on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-recurrent MDN log-likelihood on the test set: -11044.51953125\n"
     ]
    }
   ],
   "source": [
    "test_ll = model.get_loglikelihood(X=X_test_tensor, U=U_test_tensor, mask=N_t_test).sum()\n",
    "print(f'Non-recurrent MDN log-likelihood on the test set: {test_ll.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
