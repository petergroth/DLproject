{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "#import pyro\n",
    "#import pyro.distributions as dist\n",
    "#from pyro.infer import SVI, TraceEnum_ELBO, Predictive, NUTS, MCMC, config_enumerate\n",
    "#from pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal, AutoMultivariateNormal\n",
    "#from pyro.optim import Adam, ClippedAdam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import MultivariateNormal as MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "X_train_tensor, X_val_tensor, X_test_tensor =   torch.load(\"./data/X_train_tensor.pt\"), \\\n",
    "                                                torch.load(\"./data/X_val_tensor.pt\"),   \\\n",
    "                                                torch.load(\"./data/X_test_tensor.pt\")\n",
    "U_train_tensor, U_val_tensor, U_test_tensor =   torch.load(\"./data/U_train_tensor.pt\"), \\\n",
    "                                                torch.load(\"./data/U_val_tensor.pt\"),   \\\n",
    "                                                torch.load(\"./data/U_test_tensor.pt\")\n",
    "N_t_train, N_t_valid, N_t_test = np.load(\"./data/N_t_train.npy\"), \\\n",
    "                                 np.load(\"./data/N_t_val.npy\"),   \\\n",
    "                                 np.load(\"./data/N_t_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for extracting data\n",
    "\n",
    "class RFNDataset(Dataset):\n",
    "    \"\"\"Spatio-temporal demand modelling dataset.\"\"\"\n",
    "    def __init__(self, X, U):\n",
    "        self.X = X\n",
    "        self.U = U\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        X_i, U_i = self.X[idx].float(), self.U[idx].float()\n",
    "        return X_i, U_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Mixture Density network\n",
    "\n",
    "class RMDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Pure PyTorch class for Recurrent Mixture Density Network\n",
    "    Inputs:\n",
    "        input_dim:  dimension of input tensor U\n",
    "        hidden_dim: number of hidden units to be used in various hidden layers\n",
    "        output_dim: dimension of output, lat/lon\n",
    "        K: number of mixture components\n",
    "        \n",
    "    Outputs:\n",
    "        loc:    Tensor of mean values for Gaussians\n",
    "        pi:     Mixture components\n",
    "        Cov:    Covariance matrices\n",
    "        hidden: Hidden states\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, K=10):\n",
    "        super(RMDN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim= hidden_dim\n",
    "        self.output_dim= output_dim\n",
    "        self.K = K \n",
    "        self.tril_indices = torch.tril_indices(row=output_dim, col=output_dim, offset=-1)\n",
    "\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=1)\n",
    "        # Fully connected layer \n",
    "        self.lstm_to_hidden     = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        # Take output of fully connected layer and feed to layers for GMM components\n",
    "        self.hidden_to_loc      = nn.Linear(in_features=self.hidden_dim, out_features=self.K*self.output_dim)\n",
    "        self.hidden_to_sigma    = nn.Linear(in_features=self.hidden_dim, out_features=self.K*self.output_dim)\n",
    "        self.hidden_to_off_diag = nn.Linear(in_features=self.hidden_dim, out_features=self.K)\n",
    "        self.hidden_to_mix      = nn.Linear(in_features=self.hidden_dim, out_features=self.K)\n",
    "        \n",
    "        # Functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, U, hidden):     \n",
    "        # Feed through LSTM\n",
    "        y, hidden = self.lstm(U.view(-1, 1, self.input_dim), hidden)\n",
    "        # Fully connected\n",
    "        y = self.relu(self.lstm_to_hidden(y))\n",
    "        # Compute mean values\n",
    "        loc   = self.hidden_to_loc(y).view(-1, self.K, self.output_dim)\n",
    "        # Compute variances (must be positive)\n",
    "        sigma = self.softplus(self.hidden_to_sigma(y)).view(-1, self.K, self.output_dim)\n",
    "        # Compute covariances\n",
    "        cov   = self.hidden_to_off_diag(y).view(-1, self.K, 1)\n",
    "        # Compute mixture components (must sum to 1)\n",
    "        pi    = self.softmax(self.hidden_to_mix(y).view(-1,self.K, 1))\n",
    "        # Create full covariance matrix\n",
    "        cov_tril = torch.zeros((U.shape[0], self.K, self.output_dim, self.output_dim))\n",
    "        for i in range(self.K):\n",
    "            cov_tril[:, i, self.tril_indices[0], self.tril_indices[1]] = cov[:, i, :]\n",
    "            cov_tril[:, i] += torch.diag_embed(sigma[:, i, :])\n",
    "        \n",
    "        return (loc, pi, cov_tril), hidden\n",
    "    \n",
    "    \n",
    "    def get_loglikelihood(self, U, X, mask):\n",
    "        \n",
    "        logprob = 0\n",
    "        T_max = U.size(1)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.init_hidden()\n",
    "            for t in range(0, T_max):\n",
    "                # Extract components for current GMM\n",
    "                (loc, pi, cov), hidden = self.forward(U=U[:, t, :, :], hidden=hidden)\n",
    "                # Compute loglikelihood for all datapoints for current time interval\n",
    "                for tt in range(mask[t]):\n",
    "                    current_prob = 0\n",
    "                    for i in range(self.K):\n",
    "                        current_prob += pi.squeeze()[i] * torch.exp(MN(loc=loc.squeeze()[i], scale_tril=cov[:, i, :, :].squeeze()).log_prob(X[:, t, tt, :]))\n",
    "                    logprob += torch.log(current_prob)\n",
    "        return logprob\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialise hidden states\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RMDN(input_dim=48*48, hidden_dim=128, output_dim=2, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-261a7e30bef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mU_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_t_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-8910816bbe8c>\u001b[0m in \u001b[0;36mget_loglikelihood\u001b[0;34m(self, U, X, mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mcurrent_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0mcurrent_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_tril\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mlogprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mhalf_log_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhalf_log_det\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_batch_mahalanobis\u001b[0;34m(bL, bx)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mflat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_L\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = c x b x n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mflat_x_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = b x n x c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mM_swap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangular_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_x_swap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = b x c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_swap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = c x b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.get_loglikelihood(U=U_test_tensor, X=X_test_tensor, mask=N_t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(model, U, X, mask, hidden=None):\n",
    "    if hidden is None:\n",
    "        hidden = model.init_hidden()\n",
    "    \n",
    "    return model.get_loglikelihood(U)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[ 0.0763, -0.0403],\n",
       "           [ 0.0314,  0.0460],\n",
       "           [ 0.0090, -0.0716],\n",
       "           [-0.0544, -0.0400],\n",
       "           [ 0.0343, -0.0209],\n",
       "           [-0.0853, -0.0392],\n",
       "           [-0.0504, -0.0655],\n",
       "           [-0.0294,  0.0762],\n",
       "           [-0.0146,  0.0440],\n",
       "           [-0.0540,  0.0273]]], grad_fn=<ViewBackward>),\n",
       "  tensor([[[0.1042],\n",
       "           [0.0952],\n",
       "           [0.1004],\n",
       "           [0.0952],\n",
       "           [0.1064],\n",
       "           [0.1012],\n",
       "           [0.0974],\n",
       "           [0.0965],\n",
       "           [0.1019],\n",
       "           [0.1016]]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[[[ 0.6735,  0.0000],\n",
       "            [ 0.0604,  0.7017]],\n",
       "  \n",
       "           [[ 0.7204,  0.0000],\n",
       "            [-0.0033,  0.6792]],\n",
       "  \n",
       "           [[ 0.7085,  0.0000],\n",
       "            [ 0.0698,  0.6797]],\n",
       "  \n",
       "           [[ 0.7022,  0.0000],\n",
       "            [-0.0272,  0.6868]],\n",
       "  \n",
       "           [[ 0.6991,  0.0000],\n",
       "            [ 0.0258,  0.6965]],\n",
       "  \n",
       "           [[ 0.7128,  0.0000],\n",
       "            [-0.1092,  0.6724]],\n",
       "  \n",
       "           [[ 0.6669,  0.0000],\n",
       "            [ 0.0454,  0.6536]],\n",
       "  \n",
       "           [[ 0.7471,  0.0000],\n",
       "            [ 0.0621,  0.7118]],\n",
       "  \n",
       "           [[ 0.7177,  0.0000],\n",
       "            [-0.0268,  0.6947]],\n",
       "  \n",
       "           [[ 0.6711,  0.0000],\n",
       "            [ 0.0065,  0.6919]]]], grad_fn=<CopySlices>)),\n",
       " (tensor([[[-0.0316,  0.0164,  0.0017, -0.0104,  0.0055, -0.0083,  0.0112,\n",
       "             0.0099,  0.0354,  0.0363, -0.0134,  0.0113, -0.0032,  0.0156,\n",
       "            -0.0083, -0.0030, -0.0091,  0.0093, -0.0185, -0.0012,  0.0115,\n",
       "            -0.0084, -0.0099,  0.0050, -0.0146, -0.0041, -0.0051,  0.0196,\n",
       "            -0.0069,  0.0002,  0.0041, -0.0067, -0.0017,  0.0358,  0.0215,\n",
       "            -0.0176, -0.0076, -0.0051,  0.0015, -0.0197,  0.0247, -0.0208,\n",
       "             0.0057,  0.0048, -0.0020,  0.0102,  0.0136,  0.0162, -0.0061,\n",
       "             0.0010, -0.0118, -0.0173, -0.0072, -0.0179, -0.0240, -0.0276,\n",
       "             0.0399,  0.0172, -0.0019, -0.0264, -0.0268, -0.0183,  0.0039,\n",
       "             0.0076,  0.0098,  0.0027,  0.0204, -0.0176, -0.0244,  0.0099,\n",
       "             0.0228, -0.0016,  0.0359,  0.0313, -0.0186, -0.0012,  0.0121,\n",
       "            -0.0254,  0.0178,  0.0118,  0.0028,  0.0250, -0.0067, -0.0064,\n",
       "             0.0131,  0.0224, -0.0247, -0.0281, -0.0063,  0.0131,  0.0091,\n",
       "            -0.0002,  0.0073,  0.0325,  0.0240, -0.0018,  0.0179,  0.0252,\n",
       "            -0.0207,  0.0111, -0.0024,  0.0003,  0.0059,  0.0142, -0.0186,\n",
       "            -0.0371,  0.0147, -0.0272, -0.0076,  0.0335,  0.0081,  0.0236,\n",
       "             0.0031,  0.0015, -0.0133,  0.0094, -0.0201,  0.0042,  0.0008,\n",
       "            -0.0103,  0.0086,  0.0305,  0.0016,  0.0119, -0.0208,  0.0097,\n",
       "             0.0024, -0.0035]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.0622,  0.0339,  0.0037, -0.0210,  0.0115, -0.0168,  0.0222,\n",
       "             0.0193,  0.0663,  0.0696, -0.0254,  0.0238, -0.0065,  0.0319,\n",
       "            -0.0172, -0.0063, -0.0191,  0.0189, -0.0370, -0.0026,  0.0247,\n",
       "            -0.0171, -0.0200,  0.0097, -0.0284, -0.0083, -0.0109,  0.0396,\n",
       "            -0.0140,  0.0003,  0.0083, -0.0131, -0.0035,  0.0775,  0.0413,\n",
       "            -0.0355, -0.0152, -0.0103,  0.0029, -0.0398,  0.0489, -0.0424,\n",
       "             0.0114,  0.0100, -0.0040,  0.0206,  0.0261,  0.0334, -0.0129,\n",
       "             0.0020, -0.0253, -0.0329, -0.0143, -0.0355, -0.0468, -0.0571,\n",
       "             0.0804,  0.0329, -0.0039, -0.0497, -0.0546, -0.0367,  0.0078,\n",
       "             0.0148,  0.0182,  0.0059,  0.0384, -0.0348, -0.0507,  0.0197,\n",
       "             0.0456, -0.0032,  0.0775,  0.0616, -0.0373, -0.0023,  0.0234,\n",
       "            -0.0521,  0.0368,  0.0233,  0.0061,  0.0470, -0.0128, -0.0136,\n",
       "             0.0256,  0.0432, -0.0500, -0.0547, -0.0128,  0.0271,  0.0188,\n",
       "            -0.0005,  0.0141,  0.0623,  0.0476, -0.0038,  0.0363,  0.0491,\n",
       "            -0.0417,  0.0220, -0.0049,  0.0005,  0.0112,  0.0282, -0.0387,\n",
       "            -0.0712,  0.0304, -0.0505, -0.0143,  0.0703,  0.0169,  0.0480,\n",
       "             0.0061,  0.0032, -0.0282,  0.0193, -0.0391,  0.0088,  0.0017,\n",
       "            -0.0208,  0.0165,  0.0605,  0.0033,  0.0222, -0.0388,  0.0183,\n",
       "             0.0050, -0.0067]]], grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(U=U[:, 0, :, :], hidden=model.init_hidden())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 302, 48, 48])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = RFNDataset(X_train_tensor, U_train_tensor)\n",
    "dataloader= DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RMDN(input_dim=48*48, hidden_dim=128, output_dim=2, K=10)\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_ll = []\n",
    "validation_ll =  []\n",
    "for i in range(num_epochs):\n",
    "    for X_i, U_i in dataloader:\n",
    "        # Set model to training\n",
    "        pyronet.train()\n",
    "        # Take step and update parameters\n",
    "        loss = svi.step(X_i, U_i, N_t_train, hidden) /(N_t_train.sum())\n",
    "        # Save current loss\n",
    "        train_losses.append(loss)       \n",
    "        if i%100 == 99 or i == 0:\n",
    "            # Set model to evaluation\n",
    "            pyronet.eval()\n",
    "            # Compute and save training and validation log likelihood\n",
    "            train_ll_i      = pyronet.model(X=X_i, U=U_i, mask=N_t_train, validation=True)\n",
    "            validation_ll_i = pyronet.model(X=X_val_tensor, U=U_val_tensor, mask=N_t_valid, validation=True)\n",
    "            train_ll.append(train_ll_i)\n",
    "            validation_ll.append(validation_ll_i)\n",
    "            \n",
    "            print(f\"Epoch: {i+1}, \\tLoss: {loss:.3f}, \\tTraining LL: {train_ll_i.numpy()[0]},\\tValidation LL: {validation_ll_i.numpy()[0]}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
