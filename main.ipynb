{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Mixture Density Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a recurrent mixture density network (RMDN) is trained and evaluated. A simple Gaussian mixture model (GMM) will be trained as a simple baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Misc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "\n",
    "# Visualiation tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "# Pyro/PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Predictive, NUTS, MCMC, config_enumerate\n",
    "from pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "from pyro.distributions import MultivariateNormal as MN\n",
    "from pyro.ops.indexing import Vindex\n",
    "\n",
    "# Implemented modules\n",
    "from util  import *\n",
    "from model import RMDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "X_train_tensor, X_val_tensor, X_test_tensor =   torch.load(\"./data/X_train_tensor_1h.pt\"), \\\n",
    "                                                torch.load(\"./data/X_val_tensor_1h.pt\"),   \\\n",
    "                                                torch.load(\"./data/X_test_tensor_1h.pt\")\n",
    "U_train_tensor, U_val_tensor, U_test_tensor =   torch.load(\"./data/U_train_tensor_1h.pt\"), \\\n",
    "                                                torch.load(\"./data/U_val_tensor_1h.pt\"),   \\\n",
    "                                                torch.load(\"./data/U_test_tensor_1h.pt\")\n",
    "N_t_train, N_t_valid, N_t_test = np.load(\"./data/N_t_train_1h.npy\"), \\\n",
    "                                 np.load(\"./data/N_t_val_1h.npy\"),   \\\n",
    "                                 np.load(\"./data/N_t_test_1h.npy\")\n",
    "\n",
    "# Concatenation\n",
    "X_train_val = torch.cat((X_train_tensor, X_val_tensor), 1)\n",
    "U_train_val = torch.cat((U_train_tensor, U_val_tensor), 1)\n",
    "N_train_val = np.hstack((N_t_train, N_t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and train baseline GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GMM will now be trained. For this purpose, the data containing the traning data is concatenated one the same dimension to remove the temporal aspect. The data is then gridded at the same resolution as the RMDN will be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training LL:\t-27827.3462\n",
      "Baseline validation LL:\t-15790.4107\n",
      "Baseline testing LL:\t-13425.0990\n"
     ]
    }
   ],
   "source": [
    "# Create input for mixture model\n",
    "X_train_full = np.zeros((np.sum(N_t_train), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_train):\n",
    "    X_train_full[prev_count:(prev_count+count), :] = X_train_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "    \n",
    "# Create validation set\n",
    "X_val_full = np.zeros((np.sum(N_t_valid), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_valid):\n",
    "    X_val_full[prev_count:(prev_count+count), :] = X_val_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "\n",
    "# Create test set\n",
    "X_test_full = np.zeros((np.sum(N_t_test), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_test):\n",
    "    X_test_full[prev_count:(prev_count+count), :] = X_test_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count \n",
    "\n",
    "# Compute boundaries\n",
    "latmax = (latMax-latmean)/latstd\n",
    "latmin = (latMin-latmean)/latstd\n",
    "lonmax = (lonMax-lonmean)/lonstd\n",
    "lonmin = (lonMin-lonmean)/lonstd\n",
    "\n",
    "# Bin the training data\n",
    "bins_lat = np.linspace(latmin, latmax, 33)\n",
    "bins_lon = np.linspace(lonmin, lonmax, 33)\n",
    "binidx_lat = np.digitize(X_train_full[:,1], bins=bins_lat)\n",
    "binidx_lon = np.digitize(X_train_full[:,0], bins=bins_lon)\n",
    "X_train_base = np.vstack((bins_lon[binidx_lon], bins_lat[binidx_lat]))    \n",
    "    \n",
    "# Initialise and fit model\n",
    "baseline = GaussianMixture(n_components=15).fit(X_train_base.T)\n",
    "\n",
    "# Compute loglikelihood\n",
    "baseline_ll_train = baseline.score(X_train_full) * np.sum(N_t_train)\n",
    "baseline_ll_validation = baseline.score(X_val_full) * np.sum(N_t_valid)\n",
    "baseline_ll_test = baseline.score(X_test_full) * np.sum(N_t_test)\n",
    "print(f'Baseline training LL:\\t{baseline_ll_train:.4f}')\n",
    "print(f'Baseline validation LL:\\t{baseline_ll_validation:.4f}')\n",
    "print(f'Baseline testing LL:\\t{baseline_ll_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "dataset  = RFNDataset(X_train_tensor, U_train_tensor)\n",
    "dataloader= DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "grid = U_train_tensor.size(3)\n",
    "# Initialise model and guide\n",
    "model = RMDN(input_dim=grid, hidden_dim=128, LSTM_input=32, output_dim=2, LSTM_dim=32, K=30, use_cuda=False)\n",
    "guide = model.guide \n",
    "# Optimisation options\n",
    "optimizer = pyro.optim.ClippedAdam({\"lr\":3e-4, \"clip_norm\":5., \"weight_decay\":5e-2})\n",
    "svi = SVI(model.model, guide, optimizer, TraceEnum_ELBO(num_particles=1, max_plate_nesting=2))\n",
    "best_ll = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, \tLoss: 3.108, \tTrain LL: -31796.5390625, \tValidation LL: -18036.07421875\n",
      "Epoch: 500, \tLoss: 2.509, \tTrain LL: -25021.865234375, \tValidation LL: -14326.341796875\n",
      "Epoch: 1000, \tLoss: 2.432, \tTrain LL: -24171.251953125, \tValidation LL: -14030.75390625\n",
      "Epoch: 1500, \tLoss: 2.387, \tTrain LL: -23862.171875, \tValidation LL: -13934.9619140625\n",
      "Epoch: 2000, \tLoss: 2.368, \tTrain LL: -23598.068359375, \tValidation LL: -13823.349609375\n",
      "Epoch: 2500, \tLoss: 2.349, \tTrain LL: -23511.9140625, \tValidation LL: -13858.078125\n",
      "Epoch: 3000, \tLoss: 2.348, \tTrain LL: -23363.5703125, \tValidation LL: -13847.8564453125\n",
      "Epoch: 3500, \tLoss: 2.333, \tTrain LL: -23147.078125, \tValidation LL: -13811.8388671875\n",
      "Epoch: 4000, \tLoss: 2.324, \tTrain LL: -23024.48046875, \tValidation LL: -13789.1728515625\n",
      "Epoch: 4500, \tLoss: 2.316, \tTrain LL: -22953.037109375, \tValidation LL: -13818.490234375\n",
      "Epoch: 5000, \tLoss: 2.292, \tTrain LL: -22788.35546875, \tValidation LL: -13830.6962890625\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_epochs = 5000\n",
    "train_losses = []\n",
    "train_ll = []\n",
    "validation_ll = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for X_i, U_i in dataloader:\n",
    "        # Set model to training\n",
    "        model.train()\n",
    "        # Take step and update parameters\n",
    "        loss = svi.step(X_i, U_i, N_t_train) / (N_t_train.sum())       \n",
    "        # Save current loss\n",
    "        train_losses.append(loss)       \n",
    "        \n",
    "        if i%500== 499 or i == 0:\n",
    "            # Set model to evaluation\n",
    "            model.eval()\n",
    "            #Compute LL on training and validation set\n",
    "            train_ll_i      = model.get_loglikelihood(X=X_i, U=U_i, mask=N_t_train, U_init=None, X_init=None, mask_init=None).sum()\n",
    "            validation_ll_i = model.get_loglikelihood(X=X_val_tensor, U=U_val_tensor, mask=N_t_valid, U_init=U_i, X_init=X_i, mask_init=N_t_train).sum()\n",
    "            # Save LLs\n",
    "            train_ll.append(train_ll_i)\n",
    "            validation_ll.append(validation_ll_i)\n",
    "            \n",
    "            print(f\"Epoch: {i+1}, \\tLoss: {loss:.3f}, \\tTrain LL: {train_ll_i.numpy()}, \\tValidation LL: {validation_ll_i.numpy()}\")\n",
    "            \n",
    "            # Check if current LL best. If so, save checkpoint.\n",
    "            if validation_ll_i > best_ll:\n",
    "                best_ll = validation_ll_i\n",
    "                torch.save(model.state_dict(), 'models/RMDN_checkpoint')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save losses, LLs and model if save == True\n",
    "save = False\n",
    "if save:\n",
    "    with open(\"data/rmdn_losses.txt\", \"wb\") as fp:   \n",
    "        pickle.dump(train_losses, fp)\n",
    "\n",
    "    with open(\"data/rmdn_validation.txt\", \"wb\") as fp:\n",
    "        pickle.dump(validation_ll, fp)\n",
    "\n",
    "    with open(\"data/rmdn_train.txt\", \"wb\") as fp:\n",
    "        pickle.dump(train_ll, fp)    \n",
    "\n",
    "    torch.save(model.state_dict(), 'models/RMDN_trained')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that a model has been trained, it is here loaded for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMDN(\n",
       "  (FeatureExtractor): FeatureExtractor(\n",
       "    (input_to_hidden): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_output): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (MDN): MDN(\n",
       "    (input_to_hidden): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (hidden_to_loc): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (hidden_to_sigma): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (hidden_to_off_diag): Linear(in_features=128, out_features=30, bias=True)\n",
       "    (hidden_to_mix): Linear(in_features=128, out_features=30, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (softplus): Softplus(beta=1, threshold=20)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (LSTM): LSTM(32, 32)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and load model\n",
    "grid = U_train_tensor.size(3)\n",
    "model = RMDN(input_dim=grid, hidden_dim=128, LSTM_input=32, output_dim=2, LSTM_dim=32, K=30, use_cuda=False)\n",
    "model.load_state_dict(torch.load('models/RMDN_trained'))\n",
    "# Set to evaluation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained RMDN will now be evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMDN log-likelihood on the test set: -11305.1044921875\n"
     ]
    }
   ],
   "source": [
    "test_ll = model.get_loglikelihood(X=X_test_tensor, U=U_test_tensor, mask=N_t_test, U_init=U_train_val, X_init=X_train_val, mask_init=N_train_val).sum()\n",
    "print(f'RMDN log-likelihood on the test set: {test_ll.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
