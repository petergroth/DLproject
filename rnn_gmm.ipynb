{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Predictive, NUTS, MCMC, config_enumerate\n",
    "from pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pyro.distributions import MultivariateNormal as MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "\n",
    "X_train_tensor, X_val_tensor, X_test_tensor =   torch.load(\"./data/X_train_tensor.pt\"), \\\n",
    "                                                torch.load(\"./data/X_val_tensor.pt\"),   \\\n",
    "                                                torch.load(\"./data/X_test_tensor.pt\")\n",
    "U_train_tensor, U_val_tensor, U_test_tensor =   torch.load(\"./data/U_train_tensor.pt\"), \\\n",
    "                                                torch.load(\"./data/U_val_tensor.pt\"),   \\\n",
    "                                                torch.load(\"./data/U_test_tensor.pt\")\n",
    "N_t_train, N_t_valid, N_t_test = np.load(\"./data/N_t_train.npy\"), \\\n",
    "                                 np.load(\"./data/N_t_val.npy\"),   \\\n",
    "                                 np.load(\"./data/N_t_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for extracting data\n",
    "\n",
    "class RFNDataset(Dataset):\n",
    "    \"\"\"Spatio-temporal demand modelling dataset.\"\"\"\n",
    "    def __init__(self, X, U):\n",
    "        self.X = X\n",
    "        self.U = U\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        X_i, U_i = self.X[idx].float(), self.U[idx].float()\n",
    "        return X_i, U_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Mixture Density network\n",
    "\n",
    "class RMDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Pure PyTorch class for Recurrent Mixture Density Network\n",
    "    Inputs:\n",
    "        input_dim:  dimension of input tensor U\n",
    "        hidden_dim: number of hidden units to be used in various hidden layers\n",
    "        output_dim: dimension of output, lat/lon\n",
    "        K: number of mixture components\n",
    "        \n",
    "    Outputs:\n",
    "        loc:    Tensor of mean values for Gaussians\n",
    "        pi:     Mixture components\n",
    "        Cov:    Covariance matrices\n",
    "        hidden: Hidden states\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, K=10):\n",
    "        super(RMDN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim= hidden_dim\n",
    "        self.output_dim= output_dim\n",
    "        self.K = K \n",
    "        self.tril_indices = torch.tril_indices(row=output_dim, col=output_dim, offset=-1)\n",
    "\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=1)\n",
    "        # Fully connected layer \n",
    "        self.lstm_to_hidden     = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        # Take output of fully connected layer and feed to layers for GMM components\n",
    "        self.hidden_to_loc      = nn.Linear(in_features=self.hidden_dim, out_features=self.K*self.output_dim)\n",
    "        self.hidden_to_sigma    = nn.Linear(in_features=self.hidden_dim, out_features=self.K*self.output_dim)\n",
    "        self.hidden_to_off_diag = nn.Linear(in_features=self.hidden_dim, out_features=self.K)\n",
    "        self.hidden_to_mix      = nn.Linear(in_features=self.hidden_dim, out_features=self.K)\n",
    "        \n",
    "        # Functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, U, hidden):     \n",
    "        # Feed through LSTM\n",
    "        y, hidden = self.lstm(U.view(-1, 1, self.input_dim), hidden)\n",
    "        # Fully connected\n",
    "        y = self.relu(self.lstm_to_hidden(y))\n",
    "        # Compute mean values\n",
    "        loc   = self.hidden_to_loc(y).view(-1, self.K, self.output_dim)\n",
    "        # Compute variances (must be positive)\n",
    "        sigma = self.softplus(self.hidden_to_sigma(y)).view(-1, self.K, self.output_dim)\n",
    "        # Compute covariances\n",
    "        cov   = self.hidden_to_off_diag(y).view(-1, self.K, 1)\n",
    "        # Compute mixture components (must sum to 1)\n",
    "        pi    = self.softmax(self.hidden_to_mix(y).view(-1,self.K, 1))\n",
    "        # Create full covariance matrix\n",
    "        cov_tril = torch.zeros((U.shape[0], self.K, self.output_dim, self.output_dim))\n",
    "        for i in range(self.K):\n",
    "            cov_tril[:, i, self.tril_indices[0], self.tril_indices[1]] = cov[:, i, :]\n",
    "            cov_tril[:, i] += torch.diag_embed(sigma[:, i, :])\n",
    "        \n",
    "        return (loc, pi, cov_tril), hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialise hidden states\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def get_loglikelihood(self, U, X, mask):\n",
    "        \n",
    "        logprob = 0\n",
    "        T_max = U.size(1)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.init_hidden()\n",
    "            for t in range(0, T_max):\n",
    "                # Extract components for current GMM\n",
    "                (loc, pi, cov), hidden = self.forward(U=U[:, t, :, :], hidden=hidden)\n",
    "                # Compute loglikelihood for all datapoints for current time interval\n",
    "                for tt in range(mask[t]):\n",
    "                    current_prob = 0\n",
    "                    for i in range(self.K):\n",
    "                        current_prob += pi.squeeze()[i] * torch.exp(MN(loc=loc.squeeze()[i], scale_tril=cov[:, i, :, :].squeeze()).log_prob(X[:, t, tt, :]))\n",
    "                    logprob += torch.log(current_prob)\n",
    "        return logprob\n",
    "        \n",
    "# Pyro version    \n",
    "    \n",
    "class PyroRMDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Generative model for the above RMDN.\n",
    "    The class is used for training, sampling, and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, K, grid=48):\n",
    "        super(PyroRMDN, self).__init__()\n",
    "        # Initialise recurrent mixture density network\n",
    "        self.RMDN = RMDN(input_dim, hidden_dim, output_dim, K)\n",
    "        self.K = K\n",
    "        self.output_dim = output_dim\n",
    "        self.grid = grid        \n",
    "        \n",
    "    def model(self, X=None, U=None, mask=None, hidden=None, batch_size=1, validation=False):\n",
    "        # Number of sequences\n",
    "        N = len(U)\n",
    "        # Number of time steps\n",
    "        T_max = U.size(1)\n",
    "        # Batch size\n",
    "        b = min(N, batch_size)\n",
    "        # Dimension check\n",
    "        assert U.shape == (N, T_max, self.grid, self.grid)       \n",
    "        # Initialise sample tensor\n",
    "        x_samples = torch.zeros((b, T_max, max(mask), 2))\n",
    "        # Register with Pyro\n",
    "        pyro.module(\"PyroRMDN\",self)\n",
    "        # Main plate\n",
    "        with pyro.plate(\"data\", N, dim=-2):\n",
    "            # Iterate through each time interval\n",
    "            for t in pyro.markov(range(0, T_max)):\n",
    "                # Extract mixture components\n",
    "                (loc, pi, cov), hidden = self.RMDN(U[:, t, :, :], hidden)\n",
    "                # If model is to be evaluated\n",
    "                \n",
    "                with pyro.plate('density_%d'%t, size=mask[t], dim=-1):\n",
    "                    # Draw which component\n",
    "                    assignment = pyro.sample('assignment_%d'%t, dist.Categorical(pi.view(-1, self.K)))\n",
    "                    # Extract corresponding mean and covariance\n",
    "                    _loc = loc[:, assignment, :].view(-1, 2)\n",
    "                    _cov = cov[:, assignment, :, :].view(-1, 2, 2)\n",
    "                    # Draw sample as multivariate normal\n",
    "                    if X is None:\n",
    "                        x_samples[:, t, :mask[t]] = pyro.sample('x_%d'%t, MN(loc=_loc, scale_tril=_cov),obs=None)\n",
    "                    else:\n",
    "                        x_samples[:, t, :mask[t]] = pyro.sample('x_%d'%t, MN(loc=_loc, scale_tril=_cov),obs=X[:, t, :mask[t], :])\n",
    "\n",
    "        return x_samples\n",
    "    \n",
    "    def guide(self, X=None, U=None, mask=None, hidden=None):\n",
    "        pass\n",
    "    \n",
    "    def get_loglikelihood(self, U, X, mask):\n",
    "        return self.RMDN.get_loglikelihood(U, X, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = RFNDataset(X_train_tensor, U_train_tensor)\n",
    "dataloader= DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyronet = PyroRMDN(input_dim=48*48, hidden_dim=128, output_dim=2, K=10)\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.001})\n",
    "svi = SVI(pyronet.model, pyronet.guide, optimizer, TraceEnum_ELBO(num_particles=1, max_plate_nesting=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, \tLoss: 5.102, \tTraining LL: -100504.203125,\tValidation LL: -52669.046875\n",
      "Epoch: 100, \tLoss: 4.749, \tTraining LL: -90215.3046875,\tValidation LL: -47382.47265625\n",
      "Epoch: 200, \tLoss: 4.745, \tTraining LL: -90140.2421875,\tValidation LL: -47381.4765625\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_ll = []\n",
    "validation_ll =  []\n",
    "for i in range(num_epochs):\n",
    "    hidden = pyronet.RMDN.init_hidden()\n",
    "    for X_i, U_i in dataloader:\n",
    "        # Set model to training\n",
    "        pyronet.train()\n",
    "        # Take step and update parameters\n",
    "        loss = svi.step(X_i, U_i, N_t_train, hidden) /(N_t_train.sum())\n",
    "        # Save current loss\n",
    "        train_losses.append(loss)       \n",
    "        if i%100 == 99 or i == 0:\n",
    "            # Set model to evaluation\n",
    "            pyronet.eval()\n",
    "            # Compute and save training and validation log likelihood\n",
    "            train_ll_i      = pyronet.get_loglikelihood(X=X_i, U=U_i, mask=N_t_train)\n",
    "            validation_ll_i = pyronet.get_loglikelihood(X=X_val_tensor, U=U_val_tensor, mask=N_t_valid)\n",
    "            train_ll.append(train_ll_i)\n",
    "            validation_ll.append(validation_ll_i)\n",
    "            \n",
    "            print(f\"Epoch: {i+1}, \\tLoss: {loss:.3f}, \\tTraining LL: {train_ll_i.numpy()[0]},\\tValidation LL: {validation_ll_i.numpy()[0]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-52036.6367])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyronet.get_loglikelihood(U=U_test_tensor, X=X_test_tensor, mask=N_t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input for mixture model\n",
    "X_train_full = np.zeros((np.sum(N_t_train), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_train):\n",
    "    X_train_full[prev_count:(prev_count+count), :] = X_train_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "    \n",
    "# Create validation set\n",
    "X_val_full = np.zeros((np.sum(N_t_valid), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_valid):\n",
    "    X_val_full[prev_count:(prev_count+count), :] = X_val_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count\n",
    "\n",
    "# Create test set\n",
    "X_test_full = np.zeros((np.sum(N_t_test), 2))\n",
    "prev_count = 0\n",
    "for i, count in enumerate(N_t_test):\n",
    "    X_test_full[prev_count:(prev_count+count), :] = X_test_tensor[:, i, :count, :].numpy()[0]\n",
    "    prev_count+=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training LL:\t-70574.1042\n",
      "Baseline validation LL:\t-36432.0634\n",
      "Baseline testing LL:\t-36508.0325\n"
     ]
    }
   ],
   "source": [
    "# Train GMM model \n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Initialise and fit model\n",
    "baseline = GaussianMixture(n_components=10).fit(X_train_full)\n",
    "\n",
    "# Compute loglikelihood\n",
    "baseline_ll_train = baseline.score(X_train_full) * np.sum(N_t_train)\n",
    "baseline_ll_validation = baseline.score(X_val_full) * np.sum(N_t_valid)\n",
    "baseline_ll_test = baseline.score(X_test_full) * np.sum(N_t_test)\n",
    "print(f'Baseline training LL:\\t{baseline_ll_train:.4f}')\n",
    "print(f'Baseline validation LL:\\t{baseline_ll_validation:.4f}')\n",
    "print(f'Baseline testing LL:\\t{baseline_ll_test:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
